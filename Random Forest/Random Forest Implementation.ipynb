{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required modules\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(target,indx):  #This function calcultes the avg of the data labels for given indices passed as argument\n",
    "    if isinstance(indx,int):\n",
    "        return target[indx]\n",
    "    elif len(indx) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return sum([ target[i] for i in indx ]) / len(indx)\n",
    "\n",
    "def rss(target,indx):     #This function calculates the rss for the given data at the indices passed as argument\n",
    "    if len(indx) == 0 :\n",
    "        return 0.0\n",
    "    else:\n",
    "        mean = avg(target,indx)\n",
    "        return sum([ pow( target[i]-mean , 2.0 ) for i in indx ])\n",
    "\n",
    "#Class to represent nodes of the decision tree\n",
    "class decisionTree:\n",
    "    #There are two types of nodes.\n",
    "    #When the node is a leaf, then self.leaf = True, and the prediction is the average of the labels of the data reaching this leaf.\n",
    "    #When the node is not a leaf, then self.attr and self.split record the optimal split, and self.L, self.R are two sub-decision-trees.\n",
    "\n",
    "    def __init__(self,data,target,indx,depth):              #if you do not want to limit depth, you can set depth = len(data)\n",
    "        if depth==0:\t\t\t\t\t\t\t\t\t\t#if depth = 0, that means we don't go further down the tree\n",
    "            self.leaf = True\t\t\t\t\t\t\t\t#so it is a leaf\n",
    "            self.prediction = avg(target,indx)\t\t\t\t#and the prediction is the average of all labels in data[indx]\n",
    "        elif isinstance(indx,int):                          #the case when len(indx)==1;\n",
    "            self.leaf = True\n",
    "            self.prediction = target[indx]\n",
    "        elif len( set([target[i] for i in indx]) ) == 1:\t#when all labels in data[indx] are the same, we don't need to go further down the tree\n",
    "            self.leaf = True\t\t\t\t\t\t\t\t\n",
    "            self.prediction = target[indx[0]]\t\t\t\t#and the prediction is simply the common label value\n",
    "        else:\t\t\t\t\t\t\t\t\t\t\t\t#otherwise, we need to do splitting; computing optimal split below\n",
    "            self.leaf = False\t\t\t\t\t\t\t\t#so it is not a leaf\n",
    "            self.attr , self.split , self.L , self.R = self.generate(data,target,indx,depth)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #generate is the function below, \n",
    "    #it randomly samples the attribute and chooses the best attribute for splitting and the optimal split using lowest rss method\n",
    "    #attr stores which attribute is used to split\n",
    "    #split stores the numerical value used to split into left and right subtrees\n",
    "    #L and R are left and right subtrees\n",
    "\n",
    "    def generate(self,data,target,indx,depth):\t\t\t\t\t\t#generate the decision tree downwards\n",
    "        attr =0\n",
    "        split=0\n",
    "        L=0\n",
    "        R=0\n",
    "        p = np.shape(data)[1]\n",
    "        pn = math.ceil(p/3)\n",
    "        p_indx = np.random.choice(range(p-1), pn, replace=False)\n",
    "        opt = pow ( max(target) - min(target) , 2.0 ) * len(indx) + 1.0\n",
    "        for j in p_indx:\t\t\t\t\t\t\t\t\t#for each attribute, we search the optimal split \n",
    "            all_cuts = set( [ data[i][j] for i in indx ] )\t#we find out all possible split values of the attribute we are considering\n",
    "            for cut in all_cuts:\n",
    "                yl = [ i for i in indx if data[i][j]<=cut ]\t#yl is the list of indices to those observations where its j-th attribute value is <= cut\n",
    "                yr = [ i for i in indx if data[i][j]>cut ]\t#yr is the list of indices to those observations where its j-th attribute value is > cut\n",
    "                tmp = rss(target,yl) + rss(target,yr)\n",
    "                if tmp < opt:\n",
    "                    opt , attr , split, L , R = tmp , j , cut , yl , yr\n",
    "        return attr , split , decisionTree(data,target,L,depth-1) , decisionTree(data,target,R,depth-1)\n",
    "        #after finding the optimal split, at each child node we recursively generate a decision tree of height depth-1\n",
    "\n",
    "    #Function to traverse the decision tree and return the prediction for data passed as input\n",
    "    def predict(self,x):\n",
    "        if self.leaf == True:\n",
    "            return self.prediction\n",
    "        if (x[self.attr] <= self.split):\n",
    "            return self.L.predict(x)\n",
    "        else:\n",
    "            return self.R.predict(x)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Function to generate sample BTS based on data and value of B passed as argument\n",
    "#then it build the forest with B trees each of height h received as a argument\n",
    "#stores the tree in a datastructure\n",
    "#calculates training and test MSEs\n",
    "#returns the test and training MSEs\n",
    "def random_forest(X_train,y_train,X_test,y_test,B,h):\n",
    "    n = len(X_train)\n",
    "    trees = []\t#this is used to store the decision trees generated\n",
    "    test_mses = []\n",
    "    b_test_predictions = []\n",
    "    b_train_predictions = []\n",
    "    for j in range(B):\n",
    "        bts_indx = np.random.choice(range(n-1), n, replace=True)  #generating indices representing the BTS\n",
    "        bts_X_train = X_train[bts_indx,:]                         #obtaining the training BTS\n",
    "        bts_y_train = y_train[bts_indx]\n",
    "        n = len(bts_X_train)\n",
    "        all_indx = list(range(n))\n",
    "        trees.append( decisionTree( bts_X_train,bts_y_train,all_indx , h ) )  #generating the decision tree with the BTS of height h\n",
    "        #Making predictions on training and test sets using the tree\n",
    "        test_predictions = []\n",
    "        train_predictions = []\n",
    "        for x in X_train:\n",
    "            train_predictions.append(trees[-1].predict(x))\n",
    "        b_train_predictions.append(train_predictions)    \n",
    "        for x in X_test:\n",
    "            test_predictions.append(trees[-1].predict(x))\n",
    "        b_test_predictions.append(test_predictions)\n",
    "    #Calculating final prediction averaged over all trees    \n",
    "    avg_train_predictions = np.sum(b_train_predictions,axis=0)/len(b_train_predictions)\n",
    "    train_mse = np.sum(np.square(avg_train_predictions - y_train))/len(y_train)  #Calculating training MSE\n",
    "    #print(\"Training MSE:\",train_mse)\n",
    "    avg_test_predictions = np.sum(b_test_predictions,axis=0)/len(b_test_predictions)\n",
    "    test_mse = np.sum(np.square(avg_test_predictions - y_test))/len(y_test)      #Calculating training MSE\n",
    "    #print(\"Test MSE:\",test_mse)\n",
    "    return train_mse,test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(boston.data,boston.target,test_size=0.5,random_state=1214)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests with B=100,h=3 on Boston Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 14.628602392852027\n",
      "Training MSE: 26.000549403370538\n"
     ]
    }
   ],
   "source": [
    "B=100\n",
    "h=3\n",
    "a,b = random_forest(X_train,y_train,X_test,y_test,B,h)\n",
    "print(\"Training MSE:\",a)\n",
    "print(\"Training MSE:\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE for Variations of B for fixed h=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B = 10\n",
      "Training MSE: 15.527410410968264\n",
      "Training MSE: 26.43852599084298\n",
      "\n",
      "B = 20\n",
      "Training MSE: 13.469917616170425\n",
      "Training MSE: 27.096017176349363\n",
      "\n",
      "B = 30\n",
      "Training MSE: 14.471016661460727\n",
      "Training MSE: 25.63843419802816\n",
      "\n",
      "B = 40\n",
      "Training MSE: 14.111998986541975\n",
      "Training MSE: 26.032289824595743\n",
      "\n",
      "B = 50\n",
      "Training MSE: 14.737501220169168\n",
      "Training MSE: 27.22014006335549\n",
      "\n",
      "B = 60\n",
      "Training MSE: 14.643941321008008\n",
      "Training MSE: 27.04977854999381\n",
      "\n",
      "B = 70\n",
      "Training MSE: 15.162091004020512\n",
      "Training MSE: 26.895218406842915\n",
      "\n",
      "B = 80\n",
      "Training MSE: 15.213757027075495\n",
      "Training MSE: 27.372523411757772\n",
      "\n",
      "B = 90\n",
      "Training MSE: 13.827135834321137\n",
      "Training MSE: 25.841843244966253\n",
      "\n",
      "B = 100\n",
      "Training MSE: 14.78678009828894\n",
      "Training MSE: 27.134342963900853\n",
      "\n",
      "B = 200\n",
      "Training MSE: 14.09883989222337\n",
      "Training MSE: 26.060700488746857\n",
      "\n",
      "B = 300\n",
      "Training MSE: 14.18834254583526\n",
      "Training MSE: 26.624592439635688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for B in [10,20,30, 40, 50, 60, 70, 80, 90, 100,200,300]:\n",
    "    print(\"B =\",B)\n",
    "    a,b = random_forest(X_train,y_train,X_test,y_test,B,3)\n",
    "    print(\"Training MSE:\",a)\n",
    "    print(\"Training MSE:\",b)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE for Variations of h for fixed B=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H = 1\n",
      "Training MSE: 38.879294448435274\n",
      "Test MSE: 49.01026243854264\n",
      "H = 2\n",
      "Training MSE: 23.922147782493656\n",
      "Test MSE: 33.19774121070799\n",
      "H = 3\n",
      "Training MSE: 13.85860023348743\n",
      "Test MSE: 26.403744790292535\n",
      "H = 4\n",
      "Training MSE: 9.537953115689671\n",
      "Test MSE: 23.594376286509178\n",
      "H = 5\n",
      "Training MSE: 6.694569235468292\n",
      "Test MSE: 22.224319134908143\n",
      "H = 6\n",
      "Training MSE: 4.820174574095346\n",
      "Test MSE: 21.220117666028557\n",
      "H = 7\n",
      "Training MSE: 4.125479727179251\n",
      "Test MSE: 21.368806380000116\n",
      "H = 8\n",
      "Training MSE: 3.1584122917497486\n",
      "Test MSE: 21.039654686594158\n",
      "H = 9\n",
      "Training MSE: 3.187229771790403\n",
      "Test MSE: 20.35609327847366\n",
      "H = 10\n",
      "Training MSE: 2.9976867365729185\n",
      "Test MSE: 20.565117053339787\n",
      "H = 11\n",
      "Training MSE: 2.424579261570438\n",
      "Test MSE: 20.395300304244806\n",
      "H = 12\n",
      "Training MSE: 2.661363030052342\n",
      "Test MSE: 20.727457510958335\n",
      "H = 13\n",
      "Training MSE: 2.47767838111036\n",
      "Test MSE: 20.461821050158072\n"
     ]
    }
   ],
   "source": [
    "for h in [1,2,3,4,5,6,7,8,9,10,11,12,13]:\n",
    "    print(\"H =\",h)\n",
    "    a,b = random_forest(X_train,y_train,X_test,y_test,100,h)\n",
    "    print(\"Training MSE:\",a)\n",
    "    print(\"Training MSE:\",b)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE for variation of both B and H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.           1.          37.34982985  45.37525661]\n",
      " [ 10.           2.          24.82580911  34.04497891]\n",
      " [ 10.           3.          18.53430832  29.13113254]\n",
      " [ 10.           4.          11.38531881  26.20632225]\n",
      " [ 10.           5.           9.79364936  23.45866607]\n",
      " [ 10.           6.           6.6028667   20.39310373]\n",
      " [ 10.           7.           4.63411719  21.72997654]\n",
      " [ 10.           8.           4.60628955  22.17271663]\n",
      " [ 10.           9.           4.07609457  24.29366293]\n",
      " [ 10.          10.           3.51228908  21.22959867]\n",
      " [ 10.          11.           3.94749604  23.25969118]\n",
      " [ 10.          12.           4.6943308   24.80746398]\n",
      " [ 10.          13.           4.30106667  23.53270455]\n",
      " [ 20.           1.          39.19902862  49.60239544]\n",
      " [ 20.           2.          25.91468447  37.19983829]\n",
      " [ 20.           3.          15.80606309  27.64398649]\n",
      " [ 20.           4.           9.86258361  24.34725485]\n",
      " [ 20.           5.           6.51233995  24.40697751]\n",
      " [ 20.           6.           6.21351881  21.53337021]\n",
      " [ 20.           7.           4.45175457  21.53549043]\n",
      " [ 20.           8.           4.01290764  22.33579687]\n",
      " [ 20.           9.           4.05751368  20.36758939]\n",
      " [ 20.          10.           2.95969879  23.17025787]\n",
      " [ 20.          11.           2.68630858  19.77956733]\n",
      " [ 20.          12.           3.63770089  20.36867207]\n",
      " [ 20.          13.           3.74999003  23.36558496]\n",
      " [ 30.           1.          41.3744617   52.28175905]\n",
      " [ 30.           2.          25.86734677  35.53877967]\n",
      " [ 30.           3.          15.59899733  27.16659052]\n",
      " [ 30.           4.          10.07101749  23.64358761]\n",
      " [ 30.           5.           7.62877483  24.55583118]\n",
      " [ 30.           6.           5.73869803  21.75910466]\n",
      " [ 30.           7.           4.28015304  20.51028706]\n",
      " [ 30.           8.           3.45183074  21.43737301]\n",
      " [ 30.           9.           2.90754483  21.51560015]\n",
      " [ 30.          10.           3.57774361  21.05971985]\n",
      " [ 30.          11.           3.30144847  21.46162875]\n",
      " [ 30.          12.           2.64108326  21.57143577]\n",
      " [ 30.          13.           2.39011249  20.0667612 ]\n",
      " [ 40.           1.          36.97890263  46.42517895]\n",
      " [ 40.           2.          24.93758518  35.48569451]\n",
      " [ 40.           3.          14.10382483  26.60837549]\n",
      " [ 40.           4.          10.48811203  25.30284781]\n",
      " [ 40.           5.           6.70514174  22.09623463]\n",
      " [ 40.           6.           6.02217978  21.8384866 ]\n",
      " [ 40.           7.           4.318679    22.21816088]\n",
      " [ 40.           8.           3.49984302  21.41192002]\n",
      " [ 40.           9.           3.20227942  22.11873832]\n",
      " [ 40.          10.           2.47904054  20.36891301]\n",
      " [ 40.          11.           2.73757457  21.09698696]\n",
      " [ 40.          12.           2.59453951  22.77123363]\n",
      " [ 40.          13.           2.23109974  20.39676955]\n",
      " [ 50.           1.          39.01716647  48.60723438]\n",
      " [ 50.           2.          25.11860087  35.16160225]\n",
      " [ 50.           3.          14.56666498  25.79262484]\n",
      " [ 50.           4.           9.55461912  22.7967435 ]\n",
      " [ 50.           5.           6.96481487  22.78239712]\n",
      " [ 50.           6.           5.12485845  20.31204687]\n",
      " [ 50.           7.           4.21351827  21.4001425 ]\n",
      " [ 50.           8.           3.5564722   20.54866288]\n",
      " [ 50.           9.           2.91101197  21.11973231]\n",
      " [ 50.          10.           3.27619213  20.47745585]\n",
      " [ 50.          11.           2.95202898  21.33730302]\n",
      " [ 50.          12.           2.84477587  20.76230532]\n",
      " [ 50.          13.           2.02788936  20.74289557]\n",
      " [ 60.           1.          37.94322565  47.27146632]\n",
      " [ 60.           2.          23.91705128  33.12067453]\n",
      " [ 60.           3.          14.01399955  26.02533863]\n",
      " [ 60.           4.           9.79804694  23.21649204]\n",
      " [ 60.           5.           7.65861222  22.35892647]\n",
      " [ 60.           6.           5.02056606  22.28927623]\n",
      " [ 60.           7.           4.49728735  21.47777928]\n",
      " [ 60.           8.           3.20936286  20.11399141]\n",
      " [ 60.           9.           3.15914778  21.82464853]\n",
      " [ 60.          10.           3.03719482  21.74967884]\n",
      " [ 60.          11.           2.22226369  20.90654788]\n",
      " [ 60.          12.           2.75896565  21.32158941]\n",
      " [ 60.          13.           2.51661337  20.90626559]\n",
      " [ 70.           1.          36.52000083  45.92706908]\n",
      " [ 70.           2.          22.65348634  33.33352162]\n",
      " [ 70.           3.          15.15212479  26.53308101]\n",
      " [ 70.           4.           9.8869632   23.59978688]\n",
      " [ 70.           5.           6.20293209  22.01788221]\n",
      " [ 70.           6.           5.09787251  21.44413137]\n",
      " [ 70.           7.           4.05929023  21.43750693]\n",
      " [ 70.           8.           3.3964248   19.65916406]\n",
      " [ 70.           9.           2.4789927   20.21582766]\n",
      " [ 70.          10.           2.57935594  20.33185689]\n",
      " [ 70.          11.           3.14363015  21.6106982 ]\n",
      " [ 70.          12.           2.31832974  21.11389545]\n",
      " [ 70.          13.           3.05129167  21.38308542]\n",
      " [ 80.           1.          38.17991343  48.3552906 ]\n",
      " [ 80.           2.          24.01798181  33.97847825]\n",
      " [ 80.           3.          15.34127017  27.07018323]\n",
      " [ 80.           4.           9.33462527  22.79721852]\n",
      " [ 80.           5.           7.32966076  22.83138252]\n",
      " [ 80.           6.           4.76532587  21.64107007]\n",
      " [ 80.           7.           3.65111921  21.1204207 ]\n",
      " [ 80.           8.           3.72831564  22.44368866]\n",
      " [ 80.           9.           3.12662309  20.35282482]\n",
      " [ 80.          10.           2.60401218  20.62137075]\n",
      " [ 80.          11.           3.15629803  21.1023524 ]\n",
      " [ 80.          12.           2.97249421  20.71177316]\n",
      " [ 80.          13.           2.44809523  21.26889777]\n",
      " [ 90.           1.          40.50851087  50.93752421]\n",
      " [ 90.           2.          24.27827497  34.89363001]\n",
      " [ 90.           3.          15.16109004  27.37613329]\n",
      " [ 90.           4.           9.69923601  24.00814912]\n",
      " [ 90.           5.           6.60117131  22.50491475]\n",
      " [ 90.           6.           4.86675222  22.26544306]\n",
      " [ 90.           7.           3.95479344  21.55443814]\n",
      " [ 90.           8.           3.42177098  21.34018493]\n",
      " [ 90.           9.           3.03028632  20.69096124]\n",
      " [ 90.          10.           3.45190152  20.39795676]\n",
      " [ 90.          11.           2.33725265  22.08366062]\n",
      " [ 90.          12.           2.68167638  20.33415094]\n",
      " [ 90.          13.           2.94287702  20.97191388]\n",
      " [100.           1.          38.08394564  47.78384299]\n",
      " [100.           2.          23.59843456  33.65147162]\n",
      " [100.           3.          14.49595681  26.69635232]\n",
      " [100.           4.           9.67654265  23.82730873]\n",
      " [100.           5.           6.47120244  22.09696958]\n",
      " [100.           6.           4.89005971  20.89215447]\n",
      " [100.           7.           3.72138527  21.04641176]\n",
      " [100.           8.           3.50618864  21.67864081]\n",
      " [100.           9.           3.13843208  20.84103842]\n",
      " [100.          10.           3.30234782  20.97874001]\n",
      " [100.          11.           2.53228025  21.10041842]\n",
      " [100.          12.           2.84953499  22.03701849]\n",
      " [100.          13.           2.44905344  20.30155748]\n",
      " [200.           1.          39.17421377  49.54753284]\n",
      " [200.           2.          23.86706448  33.72442868]\n",
      " [200.           3.          14.27980312  26.02514539]\n",
      " [200.           4.           9.47729054  23.69075349]\n",
      " [200.           5.           6.66406726  22.9445622 ]\n",
      " [200.           6.           4.75513027  21.36353331]\n",
      " [200.           7.           3.88995279  20.36738997]\n",
      " [200.           8.           3.15568807  20.72274114]\n",
      " [200.           9.           2.91658832  20.38532985]\n",
      " [200.          10.           2.57085347  20.95978358]\n",
      " [200.          11.           2.353544    20.26838824]\n",
      " [200.          12.           2.27339986  20.32840765]\n",
      " [200.          13.           2.46682826  20.28170023]\n",
      " [300.           1.          38.58483698  48.51673891]\n",
      " [300.           2.          23.37103462  33.43833802]\n",
      " [300.           3.          14.6304014   27.02607252]\n",
      " [300.           4.           9.3671406   23.34941443]\n",
      " [300.           5.           6.49057477  21.6106539 ]\n",
      " [300.           6.           4.97993215  20.73471367]\n",
      " [300.           7.           3.76998441  20.75061279]\n",
      " [300.           8.           3.23662468  20.8617323 ]\n",
      " [300.           9.           3.24724877  21.50824149]\n",
      " [300.          10.           2.647406    20.28528274]\n",
      " [300.          11.           2.75608775  20.06662828]\n",
      " [300.          12.           2.53039022  20.34758465]\n",
      " [300.          13.           2.37917027  21.00269567]]\n"
     ]
    }
   ],
   "source": [
    "m = np.zeros((156,4))\n",
    "i =0\n",
    "for B in [10,20,30, 40, 50, 60, 70, 80, 90, 100,200,300]: \n",
    "    for h in [1,2,3,4,5,6,7,8,9,10,11,12,13]:\n",
    "        a,b = random_forest(X_train,y_train,X_test,y_test,B,h)\n",
    "        m[i][0] = B\n",
    "        m[i][1] = h\n",
    "        m[i][2] = a\n",
    "        m[i][3] = b\n",
    "        i = i +1\n",
    "print(m)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
